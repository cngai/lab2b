NAME: Christopher Ngai
EMAIL: cngai1223@gmail.com
ID: 404795904

********************
* CONTENTS OF FILE *
********************

SortedList.h:
    SortedList.h is a header file that describes the interfaces for linked list operations such as inserting, looking up, and deleting a linked list element. It also describes
    a function that finds the length of the doubly linked list.

SortedList.c:
    SortedList.c is a C program that implements the functions described in SortedList.h. It includes an insertion, look up, and deletion function that manipulates the elements 
    in the linked list. These functions are used in lab2_list.c by the threads that are sorting the linked list. It also implements the length function that is used in
    lab2_list.c to ensure that, at the end of the program, all elements that are created are eventually deleted.

lab2_list.c:
    lab2_list.c is a C program that also implements multithreading in order to insert, look up, and delete elements of a doubly linked list. A specified number of elements are
    initialized with random 1024 byte keys that must then be sorted in ascending order with the use of multithreading. The threads must then look up the element they just 
    inserted and delete the element. Both mutexes and spin-locks are also implemented if the user specifies to use one of the synchronization options in the parameters. These
    atomic instructions are enforced in order to make sure that only one thread is inserting, looking up, or deleting an element at a time; otherwise, synchronization issues 
    might arise. A yield option is also specified with the options to yield during insertion, deletion, or lookups. The program also notes the start and end time in order to
    gauge what the cost per operations were for each argument option. lab2_list.c also tracks the time it takes for threads to wait for a lock in order to see the performance
    of the mutex waits. This program also implements partitioned lists by specifying a certain number of lists in the argument parameters. The list elements are then placed 
    into their respective sublists based on a hash function that takes the element's key value and performs a modulo operation on it by the number of lists there are. In these
    sublists, the threads perform the list operations (insertion, look up, deletion) on the elements within the sublist, thus hopefully improving time performance since the
    sublists are shorter than a single list.

Makefile:
    Makefile is a file that implements a default, tests, profile, graphs, dist, and clean targets. The default target builds the lab2_list executable from lab2_list.c. The
    tests target runs all the specified tests in order to generate the lab2b_list.csv file. The profile target runs profiling tests to generate an execution profiling report
    called profile.out. The graphs target generates required graphs based on the data in the CSV file. The dist target creates the deliverable tarball with all the necessary
    files that need to be submitted. The clean target cleans all the files created by the dist target.

lab2b_list.csv:
    lab2_list.csv is a text file that contains all the results from the tests run by lab2_list in the specified format that the project spec described. It is used to create 
    the graphs that are created by lab2_list.gp.

profile.out:
    profile.out is an execution profiling report that shows where most of the time was spent when running the spin-lock list test with 1000 iterations and 12 threads. It also
    specifically shows which specific lines of code tkae up the most time when running the program.

lab2b_1.png:
    This is a graph that shows the throughput vs. number of threads for mutex and spin-lock synchronized list operations.

lab2b_2.png:
    This is a graph that shows the average time per mutex wait and the average time per operation for mutex-synchronized list operations.

lab2b_3.png:
    This is a graph that shows the successful iterations vs. threads for the unprotected, mutex, and spin-lock synchronization methods on partitioned lists.

lab2b_4.png:
    This is a graph that shows the throughput vs. the number of threads for mutex synchronized partitioned lists.

lab2b_5.png:
    This is a graph that shows the throughput vs. number of threads for spin-lock-synchronized partitioned lists.

lab2_list.gp:
    This is a script that generates four PNG files of graphs using the data in the lab2_list.csv file.

*************
* QUESTIONS *
*************

QUESTION 2.3.1 - Cycles in the basic list implementation:
    Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
    Why do you believe these to be the most expensive parts of the code?
    Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
    Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

    Most of the cycles in the 1 and 2-thread list tests are spent inserting, looking up, or deleting elements. I believe these are the most expensive
    parts of the code because when there are a small number of threads implemented, those few threads are executing these list operations at a
    relatively slow rate, which ends up being more expensive than threads checking if a critical section is locked or if it is spinning. I believe that
    most of the time/cycles in the high-thread spin-lock tests are being spent spin-waiting until the the threads have access to the critical sections.
    I think that most of the time/cycles is spent spin-waiting because when there are more threads, there is more competition to have acquire access
    to the critical sections, thus more threads will be waiting while one thread has sole access to perform any of the list operations. I believe that
    most of the time/cycles in the high-thread mutex tessts are spent in the critical sections performing the list operations because when a thread
    checks to see if a resource is locked and it is, the thread simply goes to sleep and doesn't use CPU cycles while waiting for the resource to be
    unlocked, thus, unlike the spin-lock tests, more time/cycles are spent performing the list operations.

 QUESTION 2.3.2 - Execution Profiling:
    Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
	Why does this operation become so expensive with large numbers of threads? 

    The lines of code that are consuming the most of the cycles when the spin-lock list exerciser implements a large number of threads are the lines that
    implement the spin lock and causes the threads to spin and wait until the resource needed is released and no longer locked. Specifically, it is the
    line "while (__sync_lock_test_and_set(&spin_val, 1));" which waits for the spin-lock to be unlocked so that another thread could enter the critical
    section. This operation becomes so expensive with large number of threads because when there are a large number of threads, more threads are
    competing for one resource, and, thus, they must spin and wait until the resource is released so that another thread can use it. Spinning and waiting
    uses a lot of CPU cycles which is very expensive to run, especially when many threads are waiting.

QUESTION 2.3.3 - Mutex Wait Time:
    Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
    Why does the average lock-wait time rise so dramatically with the number of contending threads?
    Why does the completion time per operation rise (less dramatically) with the number of contending threads?
    How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

    The average lock-wait time rises so dramatically with the number of contending threads because when more threads are implemented, there are more
    threads competing for a single resource. Subsequently, when there is more competition for a resource, the average wait time will increase because
    all the other threads have to wait until the thread using the resource is finished. On the other hand, the completion time per operation rises
    less dramatically with the number of contending threads because the completion time only accounts for the average time it takes for all the threads
    to complete the operations and does not include the time it takes for the threads to wait. Because the majority of the process time is spent waiting
    for the resource to be unlocked, the completion time doesn't rise as dramatically as the wait time because the time it takes to complete the
    operation is relatively quicker. It's possible for the wait time per operation to increase quicker than the completion time per operation because
    the completion time per operation just includes the overall time it takes for all the threads to complete the operation. However, the wait time per
    operation includes the wait time for each individual thread, and since multiple threads wait at the same time, the total wait time increases per
    thread and not per operation. Since there can be multiple threads running a single operation, the wait time per operation can increase much quicker
    than the completion time per operation.

QUESTION 2.3.4 - Performance of Partitioned Lists
    Explain the change in performance of the synchronized methods as a function of the number of lists.
    Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
    It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this
    appear to be true in the above curves? If not, explain why not.

    As the number of lists increases, the throughput for both synchronization methods increases as well. This occurs because when there are more
    sublists, there is less competition between the threads within the sublist for a resource; thus, the threads will be waiting for the resource
    for less time and they'll be spending more time completing operations. The throughput should continue increasing as the number of lists is further
    increased because each sublist will become shorter and the threads will performing operations on shorter lists. But the throughput only increases to
    a certain extent because, at some point, the sublists will become so small that increasing the number of sublists will bring no significant increase
    in throughput. This will act similarly to a single threaded process running through a single list, which will not show any significant increased
    throughput. While it seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list
    with fewer (1/N) threads, this does not appear to be true in the above curves because throughput is determined by the number of
    operations performed in a second, and while more threads may mean more contention, the N-way partitioned lists are shorter than a single list. In a
    single list with fewer threads, there is less contention and subsequently less wasted clock cycles, but the list is longer and the threads remain in
    the critical sections longer in order to perform the necessary operations. This elongated time in the critical sections decreases the 
    overall throughput, whereas in an N-way partitioned list, the sublists are shorter and the threads spend less time in the critical sections, thus 
    increasing the overall throughput. This is why, in the above curves, the throughput of partitioned lists with a greater amount of sublists and 
    threads is higher than the throughput of single lists with fewer threads.   
